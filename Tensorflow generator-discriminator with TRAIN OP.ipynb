{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 257\n",
    "embedding_size = 5\n",
    "hidden_units = 10\n",
    "batch_size = 5\n",
    "GO = 256\n",
    "END = 257\n",
    "image_dim = 28\n",
    "image_grid_flat = 784\n",
    "D_hidden_1 = 100\n",
    "D_out = 1\n",
    "generator_length = tf.constant(image_grid_flat, dtype=tf.int32, name=\"length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"generator_variables\"):\n",
    "    embeddings = tf.get_variable(\"g_embeddings\", shape=[vocab_size, embedding_size], dtype=tf.float32, \n",
    "                                 initializer=tf.random_uniform_initializer)\n",
    "    go_time_slice = tf.ones([batch_size], dtype=tf.int32) * 256\n",
    "    end_time_slice = tf.ones([batch_size], dtype=tf.int32) * 257\n",
    "    go_embedding = tf.nn.embedding_lookup(embeddings, go_time_slice)\n",
    "    end_embedding = tf.nn.embedding_lookup(embeddings, end_time_slice)\n",
    "\n",
    "    W = tf.Variable(tf.random_uniform([hidden_units, vocab_size], -1.0, 1.0), name=\"W\")\n",
    "    b = tf.Variable(tf.random_uniform([vocab_size]), name=\"b\")\n",
    "\n",
    "    cells = tf.nn.rnn_cell.BasicLSTMCell(hidden_units)\n",
    "    init_state = cells.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"generator_loop_function\"):\n",
    "    def loop_initial():\n",
    "        initial_elements_finished = (0 >= generator_length)\n",
    "        initial_input = go_embedding\n",
    "        initial_cell_state = init_state\n",
    "        initial_cell_output = None\n",
    "        initial_loop_state = None\n",
    "\n",
    "        return (initial_elements_finished,\n",
    "               initial_input,\n",
    "               initial_cell_state, initial_cell_output,\n",
    "               initial_loop_state)\n",
    "\n",
    "    def loop_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "        def get_next_input():\n",
    "            output_logits = tf.matmul(previous_output, W) + b\n",
    "            prediction = tf.argmax(output_logits, axis = 1)\n",
    "            next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "            return next_input\n",
    "\n",
    "        elements_finished = (time >= generator_length)\n",
    "        finished = tf.reduce_all(elements_finished)\n",
    "        ninput = tf.cond(finished, lambda: end_embedding, get_next_input)\n",
    "        state = previous_state\n",
    "        output = previous_output\n",
    "        loop_state = None\n",
    "\n",
    "        return (finished, ninput, state, output, loop_state)\n",
    "\n",
    "    def loop_fn(time, poutput, pstate, ploop):\n",
    "        if pstate is None:\n",
    "            return loop_initial()\n",
    "        else:\n",
    "            return loop_transition(time, poutput, pstate, ploop)\n",
    "\n",
    "with tf.name_scope(\"generator_main\"):\n",
    "    def generator():\n",
    "        outputs_ta, _, _ = tf.nn.raw_rnn(cells, loop_fn)\n",
    "\n",
    "        generator_outputs = outputs_ta.stack()\n",
    "\n",
    "        gen_max_time, gen_batch_size, gen_dim = tf.unstack(tf.shape(generator_outputs))\n",
    "        gen_outs_flats = tf.reshape(generator_outputs, (-1, gen_dim))\n",
    "        gen_logits_flat = tf.matmul(gen_outs_flats, W) + b\n",
    "        gen_logits = tf.reshape(gen_logits_flat, (gen_max_time, gen_batch_size, vocab_size))\n",
    "        predictions = tf.argmax(gen_logits, axis=2, output_type=tf.int32)\n",
    "        predictions = tf.reshape(predictions, (batch_size, image_grid_flat))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"discriminator\"):\n",
    "    x_real = tf.placeholder(\"float\", [None, image_grid_flat])\n",
    "    \n",
    "    D_W1 = tf.Variable(tf.random_uniform([image_grid_flat, D_hidden_1], -1.0, 1.0))\n",
    "    D_b1 = tf.Variable(tf.random_uniform([D_hidden_1], -1.0, 1.0))\n",
    "    \n",
    "    D_W2 = tf.Variable(tf.random_uniform([D_hidden_1, D_out], -1.0, 1.0))\n",
    "    D_b2 = tf.Variable(tf.random_uniform([D_out], -1.0, 1.0))\n",
    "    \n",
    "    theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "with tf.name_scope(\"discriminator_main\"):\n",
    "    def discriminator(x):\n",
    "        l1 = tf.nn.relu(tf.matmul(tf.cast(x, dtype=tf.float32), D_W1) + D_b1)\n",
    "        logits = tf.matmul(l1, D_W2) + D_b2\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return output, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running generator and then Full genrator-discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsample = generator()\\nreal_fake = discriminator(sample)\\n\\nans = sess.run(sample)\\nout, logit = sess.run(real_fake)\\nprint(\"Out\\n\", out)\\nprint(\"Logit\\n\", logit)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sample = generator()\n",
    "real_fake = discriminator(sample)\n",
    "\n",
    "ans = sess.run(sample)\n",
    "out, logit = sess.run(real_fake)\n",
    "print(\"Out\\n\", out)\n",
    "print(\"Logit\\n\", logit)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing one output of generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(ans[0, :].reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"running_GAN\"):\n",
    "    sampled = generator()\n",
    "    D_real, D_real_logits = discriminator(x_real)\n",
    "    D_fake, D_fake_logits = discriminator(sampled)\n",
    "\n",
    "with tf.name_scope(\"discriminator\"):\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)), name=\"D_real_loss\")\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)), name=\"D_fake_loss\")\n",
    "\n",
    "    #D_loss = D_loss_real + D_loss_fake\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)), name=\"G_loss\")\n",
    "\n",
    "with tf.name_scope(\"Get_trainable_Variables\"):\n",
    "    gen_var = [var for var in tf.trainable_variables() if \"discriminator\" not in var.name]\n",
    "    dis_var = [var for var in tf.trainable_variables() if \"discriminator\" in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()\n",
    "tf.summary.FileWriter(\"log\", g).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'g_embeddings:0' shape=(257, 5) dtype=float32_ref>\", \"<tf.Variable 'generator_variables/W:0' shape=(10, 257) dtype=float32_ref>\", \"<tf.Variable 'generator_variables/b:0' shape=(257,) dtype=float32_ref>\", \"<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(15, 40) dtype=float32_ref>\", \"<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(40,) dtype=float32_ref>\"] and loss Tensor(\"G_loss:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1764d035e3bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mD_real_solver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdis_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mD_fake_solver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_loss_fake\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdis_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mG_solver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    320\u001b[0m           \u001b[1;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m           \u001b[1;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'g_embeddings:0' shape=(257, 5) dtype=float32_ref>\", \"<tf.Variable 'generator_variables/W:0' shape=(10, 257) dtype=float32_ref>\", \"<tf.Variable 'generator_variables/b:0' shape=(257,) dtype=float32_ref>\", \"<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(15, 40) dtype=float32_ref>\", \"<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(40,) dtype=float32_ref>\"] and loss Tensor(\"G_loss:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "D_real_solver = tf.train.AdamOptimizer().minimize(D_loss_real, var_list = dis_var)\n",
    "D_fake_solver = tf.train.AdamOptimizer().minimize(D_loss_fake, var_list = dis_var)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list = gen_var)\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
